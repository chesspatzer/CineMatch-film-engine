{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-58:\n",
      "Process SpawnPoolWorker-57:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/uditsankhadasariya/opt/anaconda3/envs/myenv/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import threading, json\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Function to tokenize the title using NLTK\n",
    "def tokenize(title):\n",
    "    return word_tokenize(title.lower())\n",
    "\n",
    "# Function to write an inverted index to a file in the specified JSON format\n",
    "def write_inverted_index_to_json(inverted_index, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for term in sorted(inverted_index.keys()):\n",
    "            documents = inverted_index[term]\n",
    "            data = {\n",
    "                \"term\": term,\n",
    "                \"documents\": documents,\n",
    "                \"document_count\": len(documents)\n",
    "            }\n",
    "            json_line = json.dumps(data)\n",
    "            file.write(json_line + '\\n')\n",
    "\n",
    "# Function to process a chunk of rows and create an inverted index\n",
    "def process_chunk(chunk, chunk_id):\n",
    "    print(f\"Processing chunk {chunk_id}...\")\n",
    "    inverted_index = defaultdict(set)\n",
    "    for row in chunk:\n",
    "        tokens = tokenize(row[2].lower())  # Assuming 'primaryTitle' is the third column\n",
    "        for token in tokens:\n",
    "            if token in string.punctuation or token in stopwords.words('english'):\n",
    "                continue\n",
    "            inverted_index[token].add(row[0])  # Assuming 'tconst' is the first column\n",
    "    \n",
    "    # Convert sets to lists for better readability\n",
    "    for token in inverted_index:\n",
    "        inverted_index[token] = list(inverted_index[token])\n",
    "\n",
    "    # Write the inverted index of this chunk to a file\n",
    "    write_inverted_index_to_json(inverted_index, f'intermediate_v1/inverted_index_chunk_{chunk_id}.json')\n",
    "\n",
    "# Read the TSV file and divide it into chunks\n",
    "chunks = []\n",
    "chunk_size = 1  # Define your chunk size\n",
    "pool = Pool(cpu_count())  # Create a multiprocessing Pool\n",
    "# Assuming you've defined and incremented chunk_id somewhere\n",
    "chunk_id = 0\n",
    "results = []\n",
    "\n",
    "with open('Original_datasets/title.basics.tsv', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader, None)  # Skip the header line\n",
    "\n",
    "    chunk = []\n",
    "    for i, row in enumerate(reader):\n",
    "        if i == 2: break\n",
    "        chunk.append(row)\n",
    "        if len(chunk) == chunk_size:\n",
    "            result = pool.apply_async(process_chunk, args=(chunk, chunk_id))\n",
    "            results.append(result)\n",
    "            chunk_id += 1\n",
    "            chunk = []\n",
    "\n",
    "    if chunk:  # Process the last chunk if it has any rows\n",
    "        result = pool.apply_async(process_chunk, args=(chunk, chunk_id))\n",
    "        results.append(result)\n",
    "\n",
    "# Wait for all tasks to complete\n",
    "for result in results:\n",
    "    result.get()  # This will re-raise any exceptions that occurred\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
